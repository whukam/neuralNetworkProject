{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNWwjnQ1gtRgr9lisEzqJi7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whukam/neuralNetworkProject/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vVSZv7KeHHdw"
      },
      "outputs": [],
      "source": [
        "# Bring in the sequential api for the generator and discriminator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Bring in the layers for the neural network\n",
        "from tensorflow.keras.layers import Conv2DTranspose, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D, BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64*64*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Reshape((64, 64, 256))) \n",
        "\n",
        "    model.add(Conv2DTranspose(128, (9, 9), strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Conv2DTranspose(64, (9, 9), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Conv2DTranspose(1, (9, 9), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "    return model\n",
        "     \n"
      ],
      "metadata": {
        "id": "dV56qmpzlF5G"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator(): \n",
        "    model = Sequential()\n",
        "    \n",
        "    # First Conv Block\n",
        "    model.add(Conv2DTranspose(32, 5, input_shape = (128,128,1)))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    # Second Conv Block\n",
        "    model.add(Conv2DTranspose(64, 5))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    # Third Conv Block\n",
        "    model.add(Conv2DTranspose(128, 5))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    # Fourth Conv Block\n",
        "    model.add(Conv2DTranspose(256, 5))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    # Flatten then pass to dense layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    return model "
      ],
      "metadata": {
        "id": "kJk6bQQSmQFG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_s2h = generator()\n",
        "generator_h2s = generator()\n",
        "\n",
        "discriminator_s2h = discriminator()\n",
        "discriminator_h2s = discriminator()"
      ],
      "metadata": {
        "id": "FDGcSzbOlOq3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_s2h.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3pDkouxnXkF",
        "outputId": "59163722-842d-4881-933d-b789090349c0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 1048576)           104857600 \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 1048576)          4194304   \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_14 (LeakyReLU)  (None, 1048576)           0         \n",
            "                                                                 \n",
            " reshape_2 (Reshape)         (None, 64, 64, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_14 (Conv2D  (None, 64, 64, 128)      2654208   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 64, 64, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_15 (LeakyReLU)  (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_15 (Conv2D  (None, 128, 128, 64)     663552    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 128, 128, 64)     256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_16 (LeakyReLU)  (None, 128, 128, 64)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_16 (Conv2D  (None, 256, 256, 1)      5184      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 112,375,616\n",
            "Trainable params: 110,278,080\n",
            "Non-trainable params: 2,097,536\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator_s2h.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0eHPEAunXsk",
        "outputId": "040b08d2-fb47-4670-b6d2-e6d6e41752ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_transpose_20 (Conv2D  (None, 132, 132, 32)     832       \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_20 (LeakyReLU)  (None, 132, 132, 32)      0         \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 132, 132, 32)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_21 (Conv2D  (None, 136, 136, 64)     51264     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_21 (LeakyReLU)  (None, 136, 136, 64)      0         \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 136, 136, 64)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_22 (Conv2D  (None, 140, 140, 128)    204928    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_22 (LeakyReLU)  (None, 140, 140, 128)     0         \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 140, 140, 128)     0         \n",
            "                                                                 \n",
            " conv2d_transpose_23 (Conv2D  (None, 144, 144, 256)    819456    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_23 (LeakyReLU)  (None, 144, 144, 256)     0         \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 144, 144, 256)     0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 5308416)           0         \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 5308416)           0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 5308417   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,384,897\n",
            "Trainable params: 6,384,897\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam is going to be the optimizer for both\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# Binary cross entropy is going to be the loss for both \n",
        "from tensorflow.keras.losses import BinaryCrossentropy"
      ],
      "metadata": {
        "id": "yMwonxPnnX1K"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_obj = Adam(2e-4, beta_1=0.5) \n",
        "loss_obj = BinaryCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "j7CClKifpFge"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the base model class to subclass our training step \n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "1HLeYbZVp5Uf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class cycleGAN(Model): \n",
        "    def __init__(self, generator_s2h, discriminator_s2h, generator_h2s, discriminator_h2s, *args, **kwargs):\n",
        "        # Pass through args and kwargs to base class \n",
        "        super().__init__(*args, **kwargs)\n",
        "        \n",
        "        self.LAMBDA = 15\n",
        "\n",
        "        # Create attributes for gen and disc\n",
        "        self.generator_g = generator_s2h\n",
        "        self.generator_f = generator_h2s\n",
        "\n",
        "        self.discriminator_x = discriminator_s2h\n",
        "        self.discriminator_y = discriminator_h2s\n",
        "\n",
        "    def discriminator_loss(real, generated):\n",
        "      real_loss = loss_obj(tf.ones_like(real), real)\n",
        "      \n",
        "      generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
        "      \n",
        "      total_disc_loss = real_loss + generated_loss\n",
        "      \n",
        "      return total_disc_loss * 0.5\n",
        "    \n",
        "    def generator_loss(generated):\n",
        "      return loss_obj(tf.ones_like(generated), generated)\n",
        "\n",
        "    \n",
        "    def calc_cycle_loss(real_image, cycled_image):\n",
        "      loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "      return LAMBDA * loss1\n",
        "    \n",
        "    def identity_loss(real_image, same_image):\n",
        "      loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "      return LAMBDA * 0.5 * loss\n",
        "\n",
        "\n",
        "\n",
        "    def compile(self, opt_obj, loss_obj, **kwargs): \n",
        "        # Compile with base class\n",
        "        super().compile(*args, **kwargs)\n",
        "        \n",
        "        # Create attributes for losses and optimizers\n",
        "        self.generator_g_optimizer = opt_obj\n",
        "        self.discriminator_x_optimizer = opt_obj\n",
        "\n",
        "        self.generator_f_optimizer = opt_obj\n",
        "        self.discriminator_y_optimizer = opt_obj\n",
        "\n",
        "\n",
        "    def train_step(real_x, real_y):\n",
        "      # persistent is set to True because the tape is used more than\n",
        "      # once to calculate the gradients.\n",
        "      \n",
        "      \n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "        # Generator G translates X -> Y\n",
        "        # Generator F translates Y -> X.\n",
        "        \n",
        "        \n",
        "        fake_y = generator_g(real_x, training=True)\n",
        "        cycled_x = generator_f(fake_y, training=True)\n",
        "          \n",
        "        fake_x = generator_f(real_y, training=True)\n",
        "        cycled_y = generator_g(fake_x, training=True)\n",
        "        \n",
        "        # same_x and same_y are used for identity loss.\n",
        "        same_x = generator_f(real_x, training=True)\n",
        "        same_y = generator_g(real_y, training=True)\n",
        "        \n",
        "        disc_real_x = discriminator_x(real_x, training=True)\n",
        "        disc_real_y = discriminator_y(real_y, training=True)\n",
        "        \n",
        "        disc_fake_x = discriminator_x(fake_x, training=True)\n",
        "        disc_fake_y = discriminator_y(fake_y, training=True)\n",
        "        \n",
        "        # calculate the loss\n",
        "        gen_g_loss = generator_loss(disc_fake_y)\n",
        "        gen_f_loss = generator_loss(disc_fake_x)\n",
        "        \n",
        "        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
        "        \n",
        "        \n",
        "        # Total generator loss = adversarial loss + cycle loss\n",
        "        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
        "        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
        "        \n",
        "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
        "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
        "        \n",
        "        \n",
        "        # Calculate the gradients for generator and discriminator\n",
        "        generator_g_gradients = tape.gradient(total_gen_g_loss, generator_g.trainable_variables)\n",
        "        \n",
        "        generator_f_gradients = tape.gradient(total_gen_f_loss, generator_f.trainable_variables)\n",
        "        \n",
        "        \n",
        "        discriminator_x_gradients = tape.gradient(disc_x_loss, discriminator_x.trainable_variables)\n",
        "        \n",
        "        discriminator_y_gradients = tape.gradient(disc_y_loss, discriminator_y.trainable_variables)\n",
        "        \n",
        "        \n",
        "        # Apply the gradients to the optimizer\n",
        "        generator_g_optimizer.apply_gradients(zip(generator_g_gradients, generator_g.trainable_variables))\n",
        "        \n",
        "        generator_f_optimizer.apply_gradients(zip(generator_f_gradients, generator_f.trainable_variables))\n",
        "        \n",
        "        \n",
        "        discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients, discriminator_x.trainable_variables))\n",
        "        \n",
        "        discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients, discriminator_y.trainable_variables))\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "FqP8ehfvp6n_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instance of subclassed model\n",
        "GAN_model = cycleGAN(generator, discriminator)"
      ],
      "metadata": {
        "id": "y2KqM3bZp6pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "GAN_model.compile(opt_obj, loss_obj)"
      ],
      "metadata": {
        "id": "WSnhSb5qp6tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = GAN_model.fit(data, epochs=20)"
      ],
      "metadata": {
        "id": "W87G8Q33_Ftp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.suptitle('Loss')\n",
        "plt.plot(hist.history['d_loss'], label='d_loss')\n",
        "plt.plot(hist.history['g_loss'], label='g_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZzgHUWtn_YlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = generator.predict(human_data[23]))\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "hvAuZ4xT_ZZ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}